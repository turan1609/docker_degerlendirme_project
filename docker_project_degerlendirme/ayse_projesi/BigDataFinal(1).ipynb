{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2f7544-8e24-40b1-a685-135216225471",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 201805071 BATUHAN EMRE YEŞİLYAYLA\n",
    "# 201805050 BERK OĞUZ\n",
    "# BİG DATA FİNAL PROJESİ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2e28e9-383b-44e9-aca7-64e50514623c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /usr/local/lib/python3.11/site-packages (2.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (7.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Hostname:  1037136c7610\n",
      "IP Address:  172.19.0.2\n",
      "Computer Name:  1037136c7610\n",
      "System Info: \n",
      "  System: Linux\n",
      "  Node Name: 1037136c7610\n",
      "  Release: 6.8.0-60-generic\n",
      "  Version: #63~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Apr 22 19:00:15 UTC 2\n",
      "  Machine: x86_64\n",
      "  Processor: \n",
      "CPU Info:  \n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "!pip install psutil\n",
    "\n",
    "import platform\n",
    "import socket\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "ip_address = socket.gethostbyname(hostname)\n",
    "computer_name = platform.node()\n",
    "system_info = platform.uname()\n",
    "cpu_info = platform.processor()\n",
    "\n",
    "print(\"Hostname: \", hostname)\n",
    "print(\"IP Address: \", ip_address)\n",
    "print(\"Computer Name: \", computer_name)\n",
    "print(\"System Info: \")\n",
    "print(f\"  System: {system_info.system}\")\n",
    "print(f\"  Node Name: {system_info.node}\")\n",
    "print(f\"  Release: {system_info.release}\")\n",
    "print(f\"  Version: {system_info.version}\")\n",
    "print(f\"  Machine: {system_info.machine}\")\n",
    "print(f\"  Processor: {system_info.processor}\")\n",
    "print(\"CPU Info: \", cpu_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a5f846-892e-4128-9451-ffc12e333623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark \n",
    "\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb36ee93-7915-45d1-8082-ff00a89e1414",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: unrecognized option: p\n",
      "BusyBox v1.37.0 (2025-05-26 20:04:45 UTC) multi-call binary.\n",
      "\n",
      "Usage: ps [-o COL1,COL2=HEADER] [-T]\n",
      "\n",
      "Show list of processes\n",
      "\n",
      "\t-o COL1,COL2=HEADER\tSelect columns for display\n",
      "\t-T\t\t\tShow threads\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/07 11:58:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://1037136c7610:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Recommend System</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark Recommend System>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "appName = \"Spark Recommend System\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce4607a-a8b3-4071-a5f6-6e11a307f603",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/app/Books.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m books = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBooks.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m users = spark.read.csv(\u001b[33m'\u001b[39m\u001b[33mUsers.csv\u001b[39m\u001b[33m'\u001b[39m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m, header=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m ratings = spark.read.csv(\u001b[33m'\u001b[39m\u001b[33mRatings.csv\u001b[39m\u001b[33m'\u001b[39m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m, header=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:740\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc\u001b[39m(iterator):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/app/Books.csv."
     ]
    }
   ],
   "source": [
    "books = spark.read.csv('data/Books.csv', inferSchema=True, header=True)\n",
    "users = spark.read.csv('data/Users.csv', inferSchema=True, header=True)\n",
    "ratings = spark.read.csv('data/Ratings.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d363d23-ddbb-4e01-8b7c-908fcca3640a",
   "metadata": {},
   "source": [
    "## 1)PREPROCESSİNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e346f86-1ddc-4ee8-be54-884f0a777481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "books=books.drop(\"Image-URL-S\")\n",
    "books=books.drop(\"Image-URL-M\")\n",
    "books=books.drop(\"Image-URL-L\")\n",
    "\n",
    "books=books.dropna()\n",
    "books=books.dropDuplicates(subset=[\"Book-Title\"])\n",
    "books.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ead443-97b6-49f2-bcac-6d0a9c3b0c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "books_pd=books.toPandas()\n",
    "isbn_mapping = {isbn: index for index, isbn in enumerate(books_pd['ISBN'].unique())}\n",
    "books_pd['newISBN'] = books_pd['ISBN'].map(isbn_mapping)\n",
    "print(books_pd.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38723e44-5891-4bf9-8d5a-fe828c876f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "books_new=spark.createDataFrame(books_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996cdb7f-5ea9-4f62-a984-8c1005cd58a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "books_new.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccbcfea-eb7b-48b1-ac63-187032e52b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "books_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a9576-3435-49f6-b61e-e5c6caaedfbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "users=users.dropna()\n",
    "users=users.dropDuplicates()\n",
    "users.show(10)\n",
    "users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66166c5-eed5-4377-9b54-cb5d3a6606ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings=ratings.dropna()\n",
    "ratings=ratings.dropDuplicates()\n",
    "ratings.show(10)\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69ff3d-c909-4b14-b022-a88c1128c353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df=ratings.join(books_new,\"ISBN\")\n",
    "spark_df.show(10)\n",
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b712ea-392e-42d8-9bc7-a01b10467985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df=spark_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388d962-866b-492b-bf3d-e94b4ad290b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df=spark_df.join(users,\"User-ID\")\n",
    "spark_df.show(10)\n",
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824d6f2-4d95-4e44-8be5-9bdb99d3775d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df=spark_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cde0d-bade-4ea0-8b28-0aca9fd2b1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6983bb0-0b57-4922-9101-fe0ede52daba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_newISBN_count = books_new.select(\"newISBN\").distinct().count()\n",
    "print(\"newISBN sütunundaki benzersiz değer sayısı:\", unique_newISBN_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979a6fa-3da7-4ccc-a3f4-699434855ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cffbc6-a9fc-497d-82d9-bc6b4f382486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark_df = spark_df.filter(col(\"Book-Rating\") != 0)\n",
    "spark_df = spark_df.filter(col(\"Book-Author\") != \"Not Applicable (Na )\")\n",
    "spark_df.show(10)\n",
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd95811-249e-4b00-8e55-e4789c5a4719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "def is_valid_year(year):\n",
    "    try:\n",
    "        year = int(year)\n",
    "        return 1970 <= year <= 2023\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "is_valid_year_udf = udf(is_valid_year, BooleanType())\n",
    "\n",
    "filtered_df = spark_df.filter(is_valid_year_udf(col('Year-Of-Publication')))\n",
    "filtered_df = filtered_df.withColumn('Year-Of-Publication', col('Year-Of-Publication').cast('int'))\n",
    "filtered_rdd = filtered_df.rdd\n",
    "\n",
    "mapped_rdd = filtered_rdd.map(lambda row: (row['Year-Of-Publication'], (row['Book-Rating'], 1)))\n",
    "\n",
    "reduce_output = mapped_rdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "average_ratings_rdd = reduce_output.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "\n",
    "result = average_ratings_rdd.collect()\n",
    "\n",
    "sorted_result = sorted(result, key=lambda x: x[1], reverse=True)[:10]\n",
    "top_years_dict = {year: count for year, count in sorted_result}\n",
    "top_years_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860483f-f9cc-4e4e-953c-1b19f617e6be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df=filtered_df\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e01f5c-e994-4216-aaa7-01360aa90596",
   "metadata": {},
   "source": [
    "## 2)Visualition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60254439-6aa6-4b2e-9aa6-b969c43f2aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pandas = pd.DataFrame(list(top_years_dict.items()), columns=['Year', 'Avarage Ratings'])\n",
    "print(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d7c77-a124-411f-9b5f-8c88e4579099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x=\"Year\",y=\"Avarage Ratings\",data=df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6cdc7-e9de-4c7a-9044-edddc4416139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = spark_df.rdd\n",
    "mapped_rdd = rdd.map(lambda row: (row['Year-Of-Publication'], 1))\n",
    "reduce_output = mapped_rdd.reduceByKey(lambda x, y: x + y)\n",
    "result = reduce_output.collect()\n",
    "sorted_result = sorted(result, key=lambda x: x[1], reverse=True)[:10]\n",
    "top_years_dict_count = {year: count for year, count in sorted_result}\n",
    "top_years_dict_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47151580-559d-4aa2-a34b-d356b03eb33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_pandas = pd.DataFrame(list(top_years_dict_count.items()), columns=['Year', 'Book Count'])\n",
    "print(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a70726-6dd4-48a5-8ee8-5084ff02c124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x=\"Year\",y=\"Book Count\",data=df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39055cc4-064d-4ce6-89bf-fb95d9afae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=spark_df.groupby(\"Book-Rating\").count().toPandas()\n",
    "sns.barplot(x=output[\"Book-Rating\"],y=output[\"count\"],data=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882b9b3-de97-418e-b26f-72758594c7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "top_authors_df =books.groupby('Book-Author').count() \\\n",
    "                   .orderBy(desc('count')) \\\n",
    "                   .limit(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6f8f9-575b-472f-847c-fee89d68d799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_authors_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cd1047-f3cf-4f20-89e7-908d9d5e7c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_authors_pd = top_authors_df.limit(10).toPandas()\n",
    "print(top_authors_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478c24e-2acd-4b79-8a12-21c6ebbea8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_authors_pd[\"Book-Author\"], y=top_authors_pd[\"count\"], data=top_authors_pd)\n",
    "plt.xticks(rotation=90)  \n",
    "plt.xlabel('Yazar')\n",
    "plt.ylabel('Kitap Sayısı')\n",
    "plt.title('En Çok Kitabı Basılan Yazar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfeaa9d-450a-4b03-b81a-844d468c8252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "author_book_counts_df = spark_df.groupBy('Book-Author').count()\n",
    "\n",
    "top_authors_avg_rating_df = spark_df.join(top_authors_df, 'Book-Author') \\\n",
    "                               .groupBy('Book-Author') \\\n",
    "                               .agg(avg('Book-Rating').alias('Average-Rating'))\n",
    "top_authors_avg_rating_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a412012-0956-4edf-a6c3-19e793481694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_authors_avg_rating_pd = top_authors_avg_rating_df.limit(10).toPandas()\n",
    "top_authors_avg_rating_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbf08a-95ac-46c3-bb36-db933b1a6b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_authors_avg_rating_pd[\"Book-Author\"], y=top_authors_avg_rating_pd[\"Average-Rating\"], data=top_authors_avg_rating_pd)\n",
    "plt.xticks(rotation=90)  \n",
    "plt.xlabel('Yazar')\n",
    "plt.ylabel('Kitap Sayısı')\n",
    "plt.title('En Çok Kitabı Basılan Yazarların Puan Ortalaması')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa7c3d-959b-4455-90f8-8aa8c62c3b68",
   "metadata": {},
   "source": [
    "## 3)ALS Model ile Eğitim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963625a6-2259-41f6-bf1d-6e820bb8ce0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(train, test) = spark_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd09c8-ad02-41f1-a3cc-a7eea47d93de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e9e1a-e738-4fa4-a562-c6bd9772143a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f973cc-c478-4cdd-927e-a6d6a6d420b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dcf44a-855e-45ee-bd53-99f92af7fa8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "ranks = [10, 50, 200]\n",
    "iterations = [10, 12, 15]\n",
    "lambdas = [0.01, 0.1]\n",
    "results = []\n",
    "\n",
    "best_rmse = float(\"inf\")\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for rank in ranks:\n",
    "    for iteration in iterations:\n",
    "        for reg_param in lambdas:\n",
    "            als = ALS(maxIter=iteration, regParam=reg_param, rank=rank, seed=5071,\n",
    "                      userCol=\"User-ID\", itemCol=\"newISBN\", ratingCol=\"Book-Rating\",\n",
    "                      coldStartStrategy=\"drop\")\n",
    "\n",
    "            model = als.fit(train)\n",
    "\n",
    "            predictions = model.transform(test)\n",
    "            \n",
    "            evaluator_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Book-Rating\", predictionCol=\"prediction\")\n",
    "            evaluator_mse = RegressionEvaluator(metricName=\"mse\", labelCol=\"Book-Rating\", predictionCol=\"prediction\")\n",
    "            rmse = evaluator_rmse.evaluate(predictions)\n",
    "            mse = evaluator_mse.evaluate(predictions)\n",
    "            \n",
    "            print(f\"Parameters: Rank={rank}, Iteration={iteration}, Lambda={reg_param}\")\n",
    "            print(f\"RMSE: {rmse}, MSE: {mse}\")\n",
    "            results.append({\"rank\": rank, \"iteration\": iteration, \"lambda\": reg_param, \"rmse\": rmse})\n",
    "\n",
    "            print(\"---------------------------------------------\")\n",
    "            \n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_model = model\n",
    "                best_params = {\"rank\": rank, \"iteration\": iteration, \"lambda\": reg_param}\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best RMSE:\", best_rmse)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cde3f-0d5a-4692-93ed-5b7461e2701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(results_df.index, results_df['rmse'], color='blue')\n",
    "plt.xlabel('Model Index')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50316120-4aaf-4b06-926b-5ecc56d6b646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d989e70-51a3-497c-8474-75dc2fa77c39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "comparison_df = predictions.select(\"newISBN\",\"Book-Rating\", \"prediction\")\n",
    "\n",
    "comparison_pd = comparison_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc1a408-6b2c-46b7-9017-1a29b5cdc531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comparison_pd.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f8caf-3bfa-4a25-befc-8661516b4a62",
   "metadata": {},
   "source": [
    "## 4)Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54658f13-2f60-4b58-8eaf-a69b44e4cac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def cosineSimilarity(item_id1, item_id2, a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    cos_similarity = dot_product / (norm_a * norm_b)\n",
    "    return item_id1, item_id2, cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68946ba-b262-481d-a6c8-35cb4bc761f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_id1 = 53\n",
    "item_id2 = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f8654-ad57-47fa-86aa-bbbe31473d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_vector1 = model.itemFactors.filter(col(\"id\") == item_id1).select(\"features\").collect()[0][\"features\"]\n",
    "item_vector2 = model.itemFactors.filter(col(\"id\") == item_id2).select(\"features\").collect()[0][\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603aa2e6-0ab4-49f7-9c55-a5a31ce9b848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosineSimilarity(item_id1,item_id2,item_vector1,item_vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224011f-3d1c-45f8-820a-85103f8a047f",
   "metadata": {},
   "source": [
    "# 5)Recommend System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9ca62-c706-4bc2-8233-bb6f63a98285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def getUsersForBook(bookISBN, numUsers, Users):\n",
    "    bookUsers = model.recommendForAllItems(numUsers).filter(col(\"newISBN\") == bookISBN)\n",
    "    userDF = spark.createDataFrame(bookUsers.collect()[0].recommendations)\n",
    "    \n",
    "    userDF = userDF.join(Users, userDF[\"User-ID\"] == Users[\"User-ID\"], \"inner\") \\\n",
    "                           .select(userDF[\"User-ID\"], Users[\"Location\"], Users[\"Age\"], userDF[\"Rating\"])\n",
    "    \n",
    "    return userDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b2b72-a676-4ec5-9402-11b362d814d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topUsersForBook = getUsersForBook(146680, 10,users)\n",
    "topUsersForBook.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87bf08d-485b-4e10-8cfb-9cc7ea1c6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d88d2f-c5c4-48c3-b8dc-0c828f322371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
